HADOOP
------
SESSION 1 12TH NOV
------------------
1.what is big data ?
2.why Hadoop 
3.Drawbacks of database
4.Hadoop advantages
5.Hadoop components
6.Hadoop job market

----------------------------------------------------------------------------------------------------------------------------------------
Bigdata:KB,MB,GB      TB,PB,EB,ZB,YB,SB..................

1PB=1024TB's

SBI=>LAKHS

FB---->1 min ----

----------------------------------------------------------------------------------------------------------------------------------------
on an average
1.Google generaating -------------------->450PB/month
2.FaceBook generating-------------------->350PB/month
										  350*1024TB's


----------------------------------------------------------------------------------------------------------------------------------------
Oracle,MySQL,sqlserver------------------->OLTP--->GB's
Teradata,Natezza,Vertica----------------->OLAP--->TB's

1PB of data ----------->no tool 

----------------------------------------------------------------------------------------------------------------------------------------
Drawbacks of Databases:

1.Limited storage :GB's or TB's 
2.no parallel processing
3.if volume increases -------------------->speed decreases
   eg: select sum(amt) from sales1; ---------->1000 recs
       select sum(amt) from sales2; ---------->1Lakh recs
       select sum(amt) from sales3; ---------->1 cr recs
4.if complexity increases speed decreases	

  eg: select sum(amt) from sales1;
	  select avg(amt) from sales1;
	  select stddev(amt) from sales1;

5.RDBMS can handle only structured data.

	   
----------------------------------------------------------------------------------------------------------------------------------------
Hadoop Cluster : Group of cpu's(nodes) connected in a n/w
               There are 2 types of nodes
			   i)Master nodes or Name node
			  ii)Name node or Data node

Master node responsibilities
----------------------------
1.Task Assignment
2.Load balancing
3.Fault tolerence
4.Health monitor
			  
Slave node responsibilities
---------------------------
1.Storing and processing the data			  
			   
			   
----------------------------------------------------------------------------------------------------------------------------			   

Hadoop Advantages:

1.Unlimited data storage----->horizantally unlimited scalability
2.High speed processing------>parallel processing
3.can handle all varieties of data
4.opensource-------->No licensing required			   
			   
			   
----------------------------------------------------------------------------------------------------------------------------------------
SESSION 2 13th NOV
------------------
OLTP--->online transcational processing

Hadoop Components
-----------------
1.HDFS------------------------------>Storage
2.MapReduce------------------------->Processing
3.PIG------------------------------->Processing,PigLatin
4.Hive------------------------------>Processing,DWH,hql-------->sql
5.Sqoop---------------------------->sql+hadoop--------->import/export operations
6.NoSql: Schemaless
		 Random access
   Nosql database-------->Hbase,casandra,Mondgodb,,couvhdg,Riak,PNUT

7.HBase:
8.Flume --------> Streaming components ---->for streaming
9.Kafka---------->for Advanced streaming
10.YARN:
11.ZooKeeper
-----------------------------------------------------------------------------------------------------------------------------
Data storage and processing in Hadoop:

In Hadoop , Data stored in the form of files

Each file again sub-devided into blocks

default blocksize =64mb

Design principle w.r.to HDFS blocks

1.Every file has will have dedicated no of blocks
2.No two files data is stored in a single blocks
3.All the blocks have equal volume except the last block

----------------------------------------------------------------------------------------------------------------------------
SESSION 3 14th NOV
------------------

CASE 1 : No of blocks = No of slaves(B1,B2,B3,B4) (S1,S2,S3,S4)--->Then more parallelism
CASE 2 : No of blocks > No of slaves(B1,B2,B3,B4) (S1,S2)--->Then less parallelism
CASE 3 : More no of blocks ---> one slaves (B1,B2,B3,B4) (S1)--->Then no parallelism


Replication and Fault tolerence

4 step criteria

1.Name node select the slave which is highly configured
2.Name node select which is nearest
3.Name node select the slave which is more idle
4.Name node slect the slave with good health



----------------------------------------------------------------------------------------------------------------------------------------

SESSION 4 15th NOV
------------------

Design principules w.r.to replication

1.Replication applicable only to data but not for metadata
2.Replication applicable only for data node but not for name node
3.Maximum no of replication depends on the no of slave nodes
4.All the replicas can not be stored in the same slave

---------------------------------------------------------------------------------------------------------------------------
Configuring Block size and Replication

goto the hadoop installed directory ------->hadoop/conf/hdfs-site.xml

open this hdfs-site.xml

<configuration>
     <property>
	    <name>dfs.Block.size</name>
		<value>50mb</value>
	</property>
    <property>	
	    <name>dfs.replication</name>
		<value>4</value>
    </property>		
</configuration>

----------------------------------------------------------------------------------------------------------------------------

Note:If we are changing the block size then we need to restart the cluster,then only the changes will be applied.bcoz the
config file is read only once i.e during the start of the custer


SESSION 5 16th NOV
------------------




















-----------------------------------------------------------------------------------------------------------------------------

SESSION 8 21ST NOV
------------------
Hadoop is meant for storage and processing.For storage we are going with HDFS(Hadoop Distributed File System) and for processing
we are going with Map Reduce.

HDFS has three components

-Name Node
-Data Node
-Secondary Name Node

Map Reduce has 2 components

-Job Tracker
-Task Tracker

Within the Name Node we will have Job Tracker
Within the Data Node we will have Task Tracker
Job Tracker assigns task to Task Tracker and Task Tracker performs the tasks

-who is going to devide the file into blocks and store in slaves ?
 Name Node
-Who devides the job into tasks 
 Job Tracker
-who is going to executes the the tasks
 Task Tracker 


 
 ---------------------------------------------------------------------------------------------------------------------------------------
 SESSION 21 11th DEC
 -------------------
