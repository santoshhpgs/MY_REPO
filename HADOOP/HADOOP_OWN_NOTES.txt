HADOOP
------
SESSION 1 12TH NOV
------------------
1.what is big data ?
2.why Hadoop 
3.Drawbacks of database
4.Hadoop advantages
5.Hadoop components
6.Hadoop job market

----------------------------------------------------------------------------------------------------------------------------------------
Bigdata:KB,MB,GB      TB,PB,EB,ZB,YB,SB..................

1PB=1024TB's

SBI=>LAKHS

FB---->1 min ----

----------------------------------------------------------------------------------------------------------------------------------------
on an average
1.Google generaating -------------------->450PB/month
2.FaceBook generating-------------------->350PB/month
										  350*1024TB's


----------------------------------------------------------------------------------------------------------------------------------------
Oracle,MySQL,sqlserver------------------->OLTP--->GB's
Teradata,Natezza,Vertica----------------->OLAP--->TB's

1PB of data ----------->no tool 

----------------------------------------------------------------------------------------------------------------------------------------
Drawbacks of Databases:

1.Limited storage :GB's or TB's 
2.no parallel processing
3.if volume increases -------------------->speed decreases
   eg: select sum(amt) from sales1; ---------->1000 recs
       select sum(amt) from sales2; ---------->1Lakh recs
       select sum(amt) from sales3; ---------->1 cr recs
4.if complexity increases speed decreases	

  eg: select sum(amt) from sales1;
	  select avg(amt) from sales1;
	  select stddev(amt) from sales1;

5.RDBMS can handle only structured data.

	   
----------------------------------------------------------------------------------------------------------------------------------------
Hadoop Cluster : Group of cpu's(nodes) connected in a n/w
               There are 2 types of nodes
			   i)Master nodes or Name node
			  ii)Name node or Data node

Master node responsibilities
----------------------------
1.Task Assignment
2.Load balancing
3.Fault tolerence
4.Health monitor
			  
Slave node responsibilities
---------------------------
1.Storing and processing the data			  
			   
			   
----------------------------------------------------------------------------------------------------------------------------			   

Hadoop Advantages:

1.Unlimited data storage----->horizantally unlimited scalability
2.High speed processing------>parallel processing
3.can handle all varieties of data
4.opensource-------->No licensing required			   
			   
			   
----------------------------------------------------------------------------------------------------------------------------------------
SESSION 2 13th NOV
------------------
OLTP--->online transcational processing

Hadoop Components
-----------------
1.HDFS------------------------------>Storage
2.MapReduce------------------------->Processing
3.PIG------------------------------->Processing,PigLatin
4.Hive------------------------------>Processing,DWH,hql-------->sql
5.Sqoop---------------------------->sql+hadoop--------->import/export operations
6.NoSql: Schemaless
		 Random access
   Nosql database-------->Hbase,casandra,Mondgodb,,couvhdg,Riak,PNUT

7.HBase:
8.Flume --------> Streaming components ---->for streaming
9.Kafka---------->for Advanced streaming
10.YARN:
11.ZooKeeper
-----------------------------------------------------------------------------------------------------------------------------
Data storage and processing in Hadoop:

In Hadoop , Data stored in the form of files

Each file again sub-devided into blocks

default blocksize =64mb

Design principle w.r.to HDFS blocks

1.Every file has will have dedicated no of blocks
2.No two files data is stored in a single blocks
3.All the blocks have equal volume except the last block

----------------------------------------------------------------------------------------------------------------------------
SESSION 3 14th NOV
------------------

CASE 1 : No of blocks = No of slaves(B1,B2,B3,B4) (S1,S2,S3,S4)--->Then more parallelism
CASE 2 : No of blocks > No of slaves(B1,B2,B3,B4) (S1,S2)--->Then less parallelism
CASE 3 : More no of blocks ---> one slaves (B1,B2,B3,B4) (S1)--->Then no parallelism


Replication and Fault tolerence

4 step criteria

1.Name node select the slave which is highly configured
2.Name node select which is nearest
3.Name node select the slave which is more idle
4.Name node slect the slave with good health



----------------------------------------------------------------------------------------------------------------------------------------

SESSION 4 15th NOV
------------------

Design principules w.r.to replication

1.Replication applicable only to data but not for metadata
2.Replication applicable only for data node but not for name node
3.Maximum no of replication depends on the no of slave nodes
4.All the replicas can not be stored in the same slave

---------------------------------------------------------------------------------------------------------------------------
Configuring Block size and Replication

goto the hadoop installed directory ------->hadoop/conf/hdfs-site.xml

open this hdfs-site.xml

<configuration>
     <property>
	    <name>dfs.Block.size</name>
		<value>50mb</value>
	</property>
    <property>	
	    <name>dfs.replication</name>
		<value>4</value>
    </property>		
</configuration>

----------------------------------------------------------------------------------------------------------------------------

Note:If we are changing the block size then we need to restart the cluster,then only the changes will be applied.bcoz the
config file is read only once i.e during the start of the custer


SESSION 5 16th NOV
------------------

Hadoop Architecture
------------------

1.Storage Architectural components
2.processing Architectural components

Name node responsiblilites
--------------------------
1.To store the physical memory locations of the blocks of a file
2.By combining which physical memory locations of the block so that the file can be formulated 
3.To store metadata(data about data) information

Data node responsiblilites
--------------------------
1.Storing HDFS blocks.we can have multiple data nodes.the no of data nodes depends on the voume of the data.

Secondary name node
-------------------
if name node down then entire cluster is down then entire cluster will be in spectator mode means can not perform any
operation such as storage and processing.if name node is down then immidiately secondary name node comes into picture

Secondary name node responsibilities
------------------------------------
same as name node
but secondary name node is not 100% backup node for name node.
bcoz name node can read write and update but secondary name node can only read but cannot write or update

what happens internally within name node
----------------------------------------
Eevery node int the cluster will have 2 sections one is LFS and second one is HDFS
all the metadata info stored in HDFS of name node in form of blocks only.
this metadata also stored in temp folder of LFS
form this temp folder in a timely manner copied in to 2 special files 1.fsimage 2.edits
only name node can write in to fsimage and edits secondary name node can not write into fsimage and edits
metadata should not only stored in name node it should also be stored in non hadoop cluster(NHC) so when name node is down
and secondary name node is unable to retrive it can take metadata from NHC

processing components
---------------------
1.job tracker(jt)
2.task tracker(tt)

job tracker resides within name node
task tracker resides within data node

no of task trackers = no of data nodes

job tracker responsiblities
---------------------------
task assignment
load balancing
fault tolerence
health monitoring

task tracker responsibilities
------------------------------
executing the task given by job tracker


Hadoop versions
---------------

hadoop 0.xx
hadoop 1.xx
hadoop 2.xx(YARN)


SESSION 6 19th NOV
------------------
intoroductory demo session repeated

SESSION 7 20th NOV
------------------
intoroductory demo session repeated

SESSION 8 21th NOV
------------------
MapReduce
----------

Hadoop is meant for storage and processing.For storage we are going with HDFS(Hadoop Distributed File System) and for 
processing we are going with Map Reduce.

HDFS has three components

-Name Node
-Data Node
-Secondary Name Node

Map Reduce has 2 components

-Job Tracker
-Task Tracker

Within the Name Node we will have Job Tracker
Within the Data Node we will have Task Tracker
Job Tracker assigns task to Task Tracker and Task Tracker performs the tasks

-who is going to devide the file into blocks and store in slaves ?
 Name Node
-Who devides the job into tasks 
 Job Tracker
-who is going to executes the the tasks
 Task Tracker 
 
MapReduce is a component of hadoop designed for parallel processing
MR always allows data in (k,v) pair

(name,Ajay)
  k     v 
(age,25)  
  k   v 
  
  MR execution sub-devided into 3 phases
  
  1.Mapper phase
  2.Reducer phase

Reducrer performs 3 tasks:

Grouping---------->if key does not exists creates a new key,if key exists append the value`
sorting----------->based on the maper o/p key
Aggregation------->need to write logic


4 cases:

case 1:Single grouping single aggregation
case 2:Multi gouping single aggregation
case 3:Single gouping multi aggregation
case 4:Multi gouping multi aggregation


-----------------------------------------------------------------------------------------------------------------------------

SESSION 9 22nd NOV
------------------

case 1:Single grouping single aggregation

emp.txt

101,Miller,10000,M,11
102,Blake,20000,M,12
103,Sony,30000,F,11
104,Sita,40000,F,12
105,John,50000,M,13

select sex,sum(sal) from emp group by sex;


case 2:Multi gouping single aggregation

emp.txt

101,Miller,10000,M,11
102,Blake,20000,M,12
103,Sony,30000,F,11
104,Sita,40000,F,12
105,John,50000,M,13

select dno,sex,sum(sal) from emp group by dno,sex;



case 3:Single gouping multi aggregation

emp.txt

101,Miller,10000,M,11
102,Blake,20000,M,12
103,Sony,30000,F,11
104,Sita,40000,F,12
105,John,50000,M,13

select dno,sum(sal),avg(sal),max(sal),min(sal),count(*) from emp;


case 4:Multi gouping multi aggregation

[case 2 mapper + case 3 reducer]

select dno,sex,sum(sal),avg(sal),max(sal),min(sal),count(*) from emp group by dno,sex;

SESSION 10 23nd NOV
-------------------
 
if grouping------------>then reducer required
if no grouping---------->then no reducer

if reducer is not required we need to suspend the reducer even if reducer is suspended then default reducer is going to run

suspending default reducer also 

jobname.setNumReduceTask(0);

increasing the no of reducer

jobname.setNumReduceTask(2);


Partioner phase
---------------

No of mapper tasks == No of unique blocks




SESSION 11 24nd NOV
-------------------

Various phases of MapReduce
---------------------------
1.Mapper phase
2.Partioner phase
3.Reducer phase


hdfs------->i/p(k,v)---->Mapper-----o/p-------i/p(k,v)------->partioner phase------o/p(k,List(v))----i/p(k,List(v))--->
Reducer------>o/p(k,v)--->HDFS

All the above 3 phases are not executed parallely , they are executed sequentially
one after another because one phase o/p is given as i/p to another phase.

once mapper phase is 100% done then it executes partioner phase
if partioner phase is 100% done , then it executes Reducer phase,
if reducer phase is 100% done then it writes o/p to HDFS




